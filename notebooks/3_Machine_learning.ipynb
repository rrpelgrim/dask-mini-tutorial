{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0a81dd-4594-492f-8e8b-2330caac5a27",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dask/dask/main/docs/source/images/dask_horizontal.svg\"\n",
    "     width=\"60%\"\n",
    "     alt=\"Dask logo\\\" />\n",
    "\n",
    "# Parallel and Distributed Machine Learning\n",
    "\n",
    "The material in this notebook was based on the open-source content from [Dask's tutorial repository](https://github.com/dask/dask-tutorial) and the [Machine learning notebook](https://github.com/coiled/data-science-at-scale/blob/master/3-machine-learning.ipynb) from data science at scale from coiled\n",
    "\n",
    "So far we have seen how Dask makes data analysis scalable with parallelization via Dask DataFrames. Let's now see how [Dask-ML](https://ml.dask.org/) allows us to do machine learning in a parallel and distributed manner. Note, machine learning is really just a special case of data analysis (one that automates analytical model building), so the ðŸ’ª Dask gains ðŸ’ª we've seen will apply here as well!\n",
    "\n",
    "> If you'd like a refresher on the difference between parallel and distributed computing, [here's a good discussion on StackExchange](https://cs.stackexchange.com/questions/1580/distributed-vs-parallel-computing). You can also check out my [Beginner's Guide to Distributed Computing](https://towardsdatascience.com/the-beginners-guide-to-distributed-computing-6d6833796318).\n",
    "\n",
    "What we'll cover:\n",
    "1. Scale Scikit-Learn with Joblib+Dask (compute-bound)\n",
    "2. Scale Scikit-Learn with Dask-ML (memory-bound)\n",
    "3. Scale XGBoost with Dask\n",
    "\n",
    "\n",
    "## Types of scaling problems in machine learning\n",
    "\n",
    "There are two main types of scaling challenges you can run into in your machine learning workflow: scaling the **size of your data** and scaling the **size of your model**. That is:\n",
    "\n",
    "1. **CPU-bound problems**: Data fits in RAM, but training takes too long. Many hyperparameter combinations, a large ensemble of many models, etc.\n",
    "2. **Memory-bound problems**: Data is larger than RAM, and sampling isn't an option.\n",
    "\n",
    "Here's a handy diagram for visualizing these problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed3e5f1e-d82c-4cc5-8fb1-781ad9911674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/dask-zones.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"images/dask-zones.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9d25b-752a-4f64-a5d4-f6a1e33b1fa9",
   "metadata": {},
   "source": [
    "In the bottom-left quadrant, your datasets are not too large (they fit comfortably in RAM) and your model is not too large either. When these conditions are met, you are much better off using something like scikit-learn, XGBoost, and similar libraries. You don't need to leverage multiple machines in a distributed manner with a library like Dask-ML. However, if you are in any of the other quadrants, distributed machine learning is the way to go.\n",
    "\n",
    "Summarizing: \n",
    "\n",
    "* For in-memory problems, just use scikit-learn (or your favorite ML library).\n",
    "* For large models, use `dask_ml.joblib` and your favorite scikit-learn estimator.\n",
    "* For large datasets, use `dask_ml` estimators.\n",
    "\n",
    "## Scikit-Learn in five minutes\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/scikit_learn_logo_small.svg\" \n",
    "     width=\"30%\"\n",
    "     alt=\"sklearn logo\\\" />\n",
    "\n",
    "In this section, we'll quickly run through a typical Scikit-Learn workflow:\n",
    "\n",
    "* Load some data (in this case, we'll generate it)\n",
    "* Import the Scikit-Learn module for our chosen ML algorithm\n",
    "* Create an estimator for that algorithm and fit it with our data\n",
    "* Inspect the learned attributes\n",
    "* Check the accuracy of our model\n",
    "\n",
    "Scikit-Learn has a nice, consistent API:\n",
    "\n",
    "* You instantiate an `Estimator` (e.g. `LinearRegression`, `RandomForestClassifier`, etc.). All of the models *hyperparameters* (user-specified parameters, not the ones learned by the estimator) are passed to the estimator when it's created.\n",
    "* You call `estimator.fit(X, y)` to train the estimator.\n",
    "* Use `estimator` to inspect attributes, make predictions, etc. \n",
    "\n",
    "Here `X` is an array of *feature variables* (what you're using to predict) and `y` is an array of *target variables* (what we're trying to predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7379d",
   "metadata": {},
   "source": [
    "### Generate some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate data\n",
    "X, y = make_classification(n_samples=10000, n_features=4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbcf18d",
   "metadata": {},
   "source": [
    "**Refreshing some ML concepts**\n",
    "\n",
    "- `X` is the samples matrix (or design matrix). The size of `X` is typically (`n_samples`, `n_features`), which means that samples are represented as rows and features are represented as columns.\n",
    "- A \"feature\" (also called an \"attribute\") is a measurable property of the phenomenon we're trying to analyze. A feature for a dataset of employees might be their hire date, for example.\n",
    "- `y` are the target values, which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervized learning tasks, `y` does not need to be specified. `y` is usually 1d array where the `i`th entry corresponds to the target of the `i`th sample (row) of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85824219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at X\n",
    "X[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at y\n",
    "y[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d61bf2",
   "metadata": {},
   "source": [
    "### Fitting a SVC\n",
    "\n",
    "For this example, we will fit a [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d638ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator = SVC(random_state=0)\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068406a",
   "metadata": {},
   "source": [
    "We can inspect the learned features by taking a look a the `support_vectors_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a175852d",
   "metadata": {},
   "source": [
    "And we check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efac1bd",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "There are a few ways to learn the best *hyper*parameters while training. One is `GridSearchCV`.\n",
    "As the name implies, this does a brute-force search over a grid of hyperparameter combinations. Scikit-learn provides tools to automatically find the best parameter combinations via cross-validation (which is the \"CV\" in `GridSearchCV`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4659297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c889f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator = SVC(gamma='auto', random_state=0, probability=True)\n",
    "param_grid = {\n",
    "    'C': [0.001, 10.0],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "}\n",
    "\n",
    "# Brute-force search over a grid of hyperparameter combinations\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad134157",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228deb1b",
   "metadata": {},
   "source": [
    "## Compute Bound: Single-machine parallelism with Joblib\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/joblib_logo.svg\" \n",
    "     alt=\"Joblib logo\" \n",
    "     width=\"15%\"/>\n",
    "\n",
    "In this section we'll see how [Joblib](https://joblib.readthedocs.io/en/latest/) (\"*a set of tools to provide lightweight pipelining in Python*\") gives us parallelism on our laptop. Here's what our grid search graph would look like if we set up six training \"jobs\" in parallel:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/unmerged_grid_search_graph.svg\" \n",
    "     alt=\"grid search graph\" \n",
    "     width=\"75%\"/>\n",
    "\n",
    "With Joblib, we can say that Scikit-Learn has *single-machine* parallelism.\n",
    "\n",
    "**Any Scikit-Learn estimator that can operate in parallel exposes an `n_jobs` keyword**, which tells you how many tasks to run in parallel. Specifying `n_jobs=-1` jobs means running the maximum possible number of tasks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bead16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c73f87",
   "metadata": {},
   "source": [
    "Notice that the computation above it is faster than before. If you are running this computation on binder, you might not see a speed-up and the reason for that is that binder instances tend to have only one core with no threads so you can't see any parallelism. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6255f",
   "metadata": {},
   "source": [
    "## Compute Bound: Multi-machine parallelism with Dask\n",
    "\n",
    "In this section we'll see how Dask (plus Joblib and Scikit-Learn) gives us multi-machine parallelism. Here's what our grid search graph would look like if we allowed Dask to schedule our training \"jobs\" over multiple machines in our cluster:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/merged_grid_search_graph.svg\" \n",
    "     alt=\"merged grid search graph\" \n",
    "     width=\"100%\"/>\n",
    "     \n",
    "Dask can talk to Scikit-Learn (via Joblib) so that our *Dask cluster* is used to train a model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf776a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# create local Dask cluster with 8 workers (cores)\n",
    "client = Client(n_workers=8)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c77aa",
   "metadata": {},
   "source": [
    "**Note:** Click on Cluster Info, to see more details about the cluster. You can see the configuration of the cluster and some other specs. \n",
    "\n",
    "We can expand our problem by specifying more hyperparameters before training, and see how using `dask` as backend can help us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f1c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.1, 1.0, 2.5, 5, 10.0],\n",
    "    'kernel': ['rbf', 'poly', 'linear'],\n",
    "    'shrinking': [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854c927",
   "metadata": {},
   "source": [
    "### Dask parallel backend\n",
    "\n",
    "We can fit our estimator with multi-machine parallelism by quickly *switching to a Dask parallel backend* when using joblib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb681a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8228e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend(\"dask\", scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9194cf",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "Dask-ML developers worked with the Scikit-Learn and Joblib developers to implement a Dask parallel backend. So internally, scikit-learn now talks to Joblib, and Joblib talks to Dask, and Dask is what handles scheduling all of those tasks on multiple machines.\n",
    "\n",
    "The best parameters and best score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea35f8",
   "metadata": {},
   "source": [
    "## Memory Bound: Multi-machine parallelism with Dask-ML\n",
    "\n",
    "We have seen how to work with larger models, but sometimes you'll want to train on a larger than memory dataset. `dask-ml` has implemented estimators that work well on Dask `Arrays` and `DataFrames` that may be larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4cc947-e78f-47d1-ade4-e5053864bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.datasets import make_regression\n",
    "from dask_ml.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8855151-59bd-41a7-b304-58f110a71c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=10_000, chunks=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f20bc-15a3-4f74-a87d-8a58ee7e634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d73206-5423-44ca-9679-575f12a0b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ed34e-dab9-4519-a3c7-f78aac3c7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e7aee-3cd6-41d9-b190-19b85c1aa7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2ebcf-9db6-4816-a615-89136a9abc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0335593-5b5c-44f8-bd0f-d5a6fc636268",
   "metadata": {},
   "source": [
    "This works!\n",
    "\n",
    "But it does take a while...can we speed this up even more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103498a",
   "metadata": {},
   "source": [
    "##  Multi-machine parallelism in the cloud with Coiled\n",
    "\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/Coiled-Logo_Horizontal_RGB_Black.png\"\n",
    "     alt=\"Coiled logo\" \n",
    "     width=25%/>\n",
    "<br>\n",
    "\n",
    "In this section we'll see how Coiled allows us to solve machine learning problems with multi-machine parallelism in the cloud.\n",
    "\n",
    "Coiled, [among other things](https://coiled.io/product/), provides hosted and scalable Dask clusters. **The biggest barriers to entry for doing machine learning at scale are \"Do you have access to a cluster?\" and \"Do you know how to manage it?\"** Coiled solves both of those problems. \n",
    "\n",
    "We'll connect to the Coiled cluster we launched earlier then connect our Dask client to that cluster.\n",
    "\n",
    "If you are running on your local machine and not in binder, and you want to give Coiled a try, you can signup [here](https://cloud.coiled.io/login?redirect_uri=/) and reach out to Support to get set up with some free credits. If you installed the environment by following the steps on the repository's [README](https://github.com/coiled/dask-mini-tutorial/blob/main/README.md) you will have `coiled` installed. You will just need to login, by following the steps on the [setup page](https://docs.coiled.io/user_guide/getting_started.html), and you will be ready to go. \n",
    "\n",
    "To learn more about how to set up an environment you can visit Coiled documentation on [Creating software environments](https://docs.coiled.io/user_guide/software_environment_creation.html). But for now you can use the envioronment we set up for this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4268dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to running cluster by referencing only its name (GET NAME FROM COILED DASHBOARD)\n",
    "cluster = coiled.Cluster(\n",
    "    name=\"dask-tutorial\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62506a94-71fb-4610-8c9c-e93785fd6345",
   "metadata": {},
   "source": [
    "Our cluster currently has 20 workers.\n",
    "\n",
    "Machine Learning is a compute-heavy process and we're working against a tight deadline. Let's scale up to 100 to speed this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3914a2-9b1d-4898-945d-0921e3f03189",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ddd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3980aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Same Linear Regression Model as before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccea68",
   "metadata": {},
   "source": [
    "We can use Dask-ML estimators on the cloud to work with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114530a-11a4-47e4-b13c-5ee541b792dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=2_000_000, chunks=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90aef0-a049-473f-bdfb-528cb8134bcc",
   "metadata": {},
   "source": [
    "Notice we created a dataset with 1000 chunks, a number that can be logically distributed over our n_workers (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c280f7e-049c-40f7-a71d-9ceea493032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1aa32-80ba-44fc-b286-788e3d17a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533c497-a219-4ed5-b827-04b430a02994",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67fe898-17ec-4899-a576-089c25f9facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00eb9f-00a2-48d5-9c8a-93bf85bf0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a1447-c134-4983-9abb-eb39fbfdc6ee",
   "metadata": {},
   "source": [
    "## Training XGBoost in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e97622-b62e-4ab7-aea2-04a296af1ab5",
   "metadata": {},
   "source": [
    "Dask-ML implements some of the most popular machine learning algorithms for parallel processing, but not all of them.\n",
    "\n",
    "For XGBoost, the maintainers of Dask and XGBoost took a different approach: they built a Dask Backend for XGBoost so you can run XGBoost in parallel with Dask straight from your normal XGBoost library.\n",
    "\n",
    "Running an XGBoost model with the distributed Dask backend only requires two changes to your regular XGBoost code:\n",
    "\n",
    "1. substitute `dtrain = xgb.DMatrix(X_train, y_train)` with `dtrain = xgb.dask.DaskDMatrix(X_train, y_train)`\n",
    "2. substitute `xgb.train(params, dtrain, ...)` with `xgb.dask.train(client, params, dtrain)`\n",
    "\n",
    "[Here's a step-by-step tutorial.](https://coiled.io/blog/dask-xgboost-python-example/) that trains XGBoost on 100GB of synthetic data in 4 minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f839e",
   "metadata": {},
   "source": [
    "## Extra resources:\n",
    "\n",
    "- [Dask-ML documentation](https://ml.dask.org/)\n",
    "- [Getting started with Coiled](https://docs.coiled.io/user_guide/getting_started.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask-tutorial",
   "language": "python",
   "name": "dask-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
